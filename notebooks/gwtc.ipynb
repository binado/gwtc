{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f56ef8-a618-4a5a-9850-dfb6f3d5fe48",
   "metadata": {},
   "source": [
    "## Analyzing localization of gravitational-wave sources from GWTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f90c33-bf9e-4e06-ae54-f8e2b2b55f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import ipydatagrid\n",
    "import arviz as az\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table, QTable\n",
    "import astropy.units as u\n",
    "import h5py\n",
    "import healpy as hp\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray, ArrayLike\n",
    "from ligo.skymap.postprocess.ellipse import find_ellipse\n",
    "import ligo.skymap.plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gwtc.source import approximant_to_label, label_to_approximant\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d75e90-9954-4e05-89c2-ec549a6e3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_localization_samples = \"../parsed_samples.hdf5\"\n",
    "path_to_output_file_prefix = \"../gwtc-localization-samples-best-localized\"\n",
    "approximants = [\"IMRPhenomXPHM\", \"SEOBNRv4PHM\"]\n",
    "analyses = approximants + [\"Mixed\"]\n",
    "output_analysis = \"Mixed\"\n",
    "path_to_output_file = f\"{path_to_output_file_prefix}-analysis={output_analysis}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408d63e-6919-440e-8367-2d72c3f31184",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_file = h5py.File(path_to_localization_samples, mode=\"r\")\n",
    "events = list(samples_file.keys())\n",
    "event_table = Table({\"Events\": events})\n",
    "event_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18e33d-c678-4ea6-bf24-ae84eee5394e",
   "metadata": {},
   "source": [
    "## Per-event posterior samples\n",
    "\n",
    "### Binary black holes\n",
    "\n",
    "The [GWTC catalogue data releases](https://gwosc.org/eventapi/html/GWTC/) contain posterior samples from parameter estimation using two waveform approximants, IMPRhenomXPHM and SEOBNRv4PHM. The number of posterior samples for each event is different between the approximants, most of them coming from IMRPhenomXPHM. There is also a `Mixed` sample, which combines the results from both pipelines.\n",
    "\n",
    "In the cell below, we display the total number of samples per event, as well as the fraction of samples in each analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e67440-bc2a-46cc-b180-14ff08fa0786",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = list(samples_file.keys())\n",
    "data = []\n",
    "for event, group in samples_file.items():\n",
    "    row = {label_to_approximant(label): samples.attrs[\"nsamples\"] for label, samples in group.items()}\n",
    "    total_samples = sum(row.values())\n",
    "    data.append({\n",
    "        \"Total samples\": total_samples, \n",
    "        **{k: v / total_samples for k,v in row.items()}\n",
    "    })\n",
    "\n",
    "# Reverse list of dicts into dict of lists\n",
    "cols = list(data[0].keys())\n",
    "data = {col: [row.get(col, np.nan) for row in data] for col in cols}\n",
    "\n",
    "for colname, col in data.items():\n",
    "    event_table[colname] = col\n",
    "\n",
    "def format_columns(table, float_fmt=\"1.2f\", int_fmt=\"1.1e\"):\n",
    "    for column in table.colnames:\n",
    "        if table[column].dtype == np.int64 and int_fmt is not None:\n",
    "            table[column].info.format = int_fmt\n",
    "        if table[column].dtype == np.float64 and float_fmt is not None:\n",
    "            table[column].info.format = float_fmt\n",
    "\n",
    "def get_renderer_by_format(fmt):\n",
    "    return ipydatagrid.TextRenderer(\n",
    "        text_value=ipydatagrid.VegaExpr(f\"format(cell.value, '{fmt}')\")\n",
    "    )\n",
    "\n",
    "def pretty_show_in_notebook(table, float_fmt=\"1.2f\", int_fmt=\"1.1e\"):\n",
    "    \"\"\"\n",
    "    Use astropy Table's `show_in_notebook` method to display a dynamic table\n",
    "    on a notebook, and customize the formatting of floats and ints.\n",
    "    \"\"\"\n",
    "    renderers = {}\n",
    "    float_renderer = get_renderer_by_format(float_fmt)\n",
    "    int_renderer = get_renderer_by_format(int_fmt)\n",
    "    for col in table.colnames:\n",
    "        if table[col].dtype == np.float64 and float_fmt is not None:\n",
    "            renderers[col] = float_renderer\n",
    "        elif table[col].dtype == np.int64 and int_fmt is not None:\n",
    "            renderers[col] = int_renderer\n",
    "    return table.show_in_notebook(renderers=renderers, auto_fit_columns=True)\n",
    "            \n",
    "\n",
    "# Pretty printing numbers\n",
    "pretty_show_in_notebook(event_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ca480-42ab-414a-8dd9-a35601dac86b",
   "metadata": {},
   "source": [
    "## Analyzing sky localization of all events in the catalog\n",
    "\n",
    "In the following cells, we select all events in GWTC, distingushing those which were detected by at least three detectors (LIGO Hanford, LIGO Livingston and Virgo) from others which were detected while only two interferometers were operating.\n",
    "\n",
    "We expect the source localization to be more precise for the former set of events. To visualize this, we construct a table of the 90% credible localization area $\\Delta \\Omega_{90\\%}$ for each event and for both waveform approximants. The area has been calculated by the LVK collaboration and is included in the data products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e1cc9d-bf15-4f41-91d4-20daa0bf4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_detectors = {}\n",
    "\n",
    "# Find out which interferometers detected each event\n",
    "for event, group in samples_file.items():\n",
    "    detectors = []\n",
    "    # The detectors are stored in a funny way, in that they are sometimes recorded in only one analysis,\n",
    "    # despite being the same for all of them in practice. Because of this, we loop over everything\n",
    "    label_with_detectors = None\n",
    "    for label, approximant_data in group.items():\n",
    "        detectors_for_label = approximant_data.attrs[\"detectors\"]\n",
    "        if all([det is not None for det in detectors_for_label]) and len(detectors_for_label) > len(detectors):\n",
    "            detectors = detectors_for_label\n",
    "            label_with_detectors = label\n",
    "            \n",
    "    event_detectors[event] = {\"label\": label_with_detectors, \"detectors\": detectors}\n",
    "\n",
    "three_detector_events = {k for k,v in event_detectors.items() if len(v[\"detectors\"]) >= 3}\n",
    "print(f\"Number of events detected by L1 H1 V1: {len(three_detector_events)}\")\n",
    "\n",
    "def get_confidence_area_label(analysis: str):\n",
    "    return f\"[{analysis}] Confidence area (90%)\"\n",
    "\n",
    "def get_skymap_stats_for_event(event_group: h5py.Group, analysis: str) -> dict:\n",
    "    try:\n",
    "        approximant_data = event_group[approximant_to_label(analysis)]\n",
    "        area = approximant_data.attrs[\"area90\"]\n",
    "    except KeyError:\n",
    "        area = np.nan\n",
    "    return {get_confidence_area_label(analysis): area * u.degree ** 2}\n",
    "    \n",
    "\n",
    "def get_snr_stats_for_event(event: str, event_group: h5py.Group) -> dict:\n",
    "    label = event_detectors[event][\"label\"]\n",
    "    approximant_data = event_group[label]\n",
    "    for detector in [\"H1\", \"L1\", \"V1\"]:\n",
    "        try:\n",
    "            snr = approximant_data[f\"{detector}_matched_filter_abs_snr\"][()]\n",
    "            median_snr = np.median(snr)\n",
    "        except KeyError:\n",
    "            median_snr = np.nan\n",
    "        data[f\"{detector} median SNR\"] = median_snr\n",
    "    return data\n",
    "        \n",
    "\n",
    "data_list = []\n",
    "for event, group in samples_file.items():\n",
    "    data = {\"Event\": event, \"Detected by\": event_detectors[event][\"detectors\"]}\n",
    "    for approximant in approximants:\n",
    "        skymap_stats = get_skymap_stats_for_event(group, approximant)\n",
    "        data.update(skymap_stats)\n",
    "\n",
    "    snr_stats = get_snr_stats_for_event(event, group)\n",
    "    data.update(snr_stats)\n",
    "    data_list.append(data)\n",
    "\n",
    "confidence_area_labels = list(map(get_confidence_area_label, approximants))\n",
    "event_localization_table = QTable(rows=data_list)\n",
    "event_localization_table[\"Detected by\"].format = lambda x: \" \".join(x)\n",
    "format_columns(event_localization_table, float_fmt=\".1f\", int_fmt=\".1e\")\n",
    "event_localization_table.sort(confidence_area_labels)\n",
    "pretty_show_in_notebook(event_localization_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fb408-a5cb-44f9-9389-7f21ce81caf0",
   "metadata": {},
   "source": [
    "Visualizing the credible localization areas in a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a331bef-160f-4439-9ba2-c7f26c44b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "nbins = 10\n",
    "plot_labels = []\n",
    "three_detector_mask = np.asarray([len(event_det) >= 3 for event_det in event_localization_table[\"Detected by\"]])\n",
    "for mask, mask_label in [(three_detector_mask, \"three detectors\"), (~three_detector_mask, \"two detectors\")]:\n",
    "    for approximant, label in zip(approximants, confidence_area_labels):\n",
    "        data = event_localization_table[label]\n",
    "        nan_mask = np.isnan(data)\n",
    "        mask = np.logical_and(~nan_mask, mask)\n",
    "        valid_data = data[mask]\n",
    "        bins = np.geomspace(valid_data.min(), valid_data.max(), nbins)\n",
    "        plot_label = f\"{approximant}, {mask_label}\"\n",
    "        ax.hist(valid_data, bins=bins, label=plot_label, histtype=\"stepfilled\", alpha=0.8)\n",
    "        plot_labels.append(plot_label)\n",
    "\n",
    "ax.set(xlabel=\"Confidence area (90%) [deg2]\", ylabel=\"Number of events\", xscale=\"log\")\n",
    "ax.grid()\n",
    "_ = fig.legend(plot_labels, loc=\"outside upper right\", ncols=len(approximants))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925bb7b-902d-4f30-ac76-e760b16f2403",
   "metadata": {},
   "source": [
    "The last two events, GW200308_173609 and GW200322_091133, seem to be very poorly localized, with $\\Delta \\Omega_{90\\%}$ almost spanning the whole sky. Even though they were detected when the whole network as operating, the network SNR for each of them is less than 5.\n",
    "\n",
    "\n",
    "\n",
    "Let us take a closer look at their posterior samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7acfb2-cb02-4788-b089-bd31cad32501",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_events = [\"GW200308_173609\", \"GW200322_091133\"]\n",
    "nrows = len(outlier_events)\n",
    "npts = 100\n",
    "#fig, axs = plt.subplots(2 * nrows, 2 * 2, figsize=(16 * nrows, 6 * 4))\n",
    "coords = {\"ra\": np.linspace(-0.5 * np.pi, 0.5 * np.pi, npts), \"dec\": np.linspace(0, 2 * np.pi)}\n",
    "for i, event in enumerate(outlier_events):\n",
    "    event_group = samples_file[event]\n",
    "    for j, approximant in enumerate(approximants):\n",
    "        label = approximant_to_label(approximant)\n",
    "        ra, dec = event_group[label][\"ra\"], event_group[label][\"dec\"][()]\n",
    "        ds = az.convert_to_inference_data({\"ra\": ra, \"dec\": dec}, coords=coords)\n",
    "        #axs_to_plot = axs[2*i:2*i+2, 2*j:2*j+2]\n",
    "        axs = az.plot_pair(ds, marginals=True)\n",
    "        fig = axs.flatten()[0].get_figure()\n",
    "        fig.suptitle(f\"{event} - {approximant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f534142-102e-4e1f-940c-1cb5c5b199c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_from_point_sources(\n",
    "    theta: NDArray,\n",
    "    phi: NDArray,\n",
    "    nside: int,\n",
    "    normalize: bool = False,\n",
    "    weights: Optional[ArrayLike] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    indices = hp.ang2pix(nside, theta, phi, **kwargs)\n",
    "    npix = hp.nside2npix(nside)\n",
    "    counts = np.bincount(indices, weights=weights, minlength=npix).astype(np.float64)\n",
    "    if not normalize:\n",
    "        return counts\n",
    "    pix_sum = np.sum(counts)\n",
    "    return counts / pix_sum if pix_sum > 0 else counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d94cb1-63ee-4988-b247-4fdd2717ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = \"GW200308_173609\"\n",
    "nside = 32\n",
    "area_per_pixel = hp.nside2pixarea(nside, degrees=True)\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "data = samples_file[event][label]\n",
    "ra, dec = data[\"ra\"][()], data[\"dec\"][()]\n",
    "\n",
    "points = np.column_stack((ra, dec))\n",
    "mean_ra, mean_dec = np.mean(ra), np.mean(dec)\n",
    "center = SkyCoord(frame=\"icrs\", ra=mean_ra, dec=mean_dec, unit=\"rad\")\n",
    "ax = fig.add_subplot(projection=\"astro degrees mollweide\")\n",
    "theta, phi = 0.5 * np.pi - dec, ra\n",
    "counts = counts_from_point_sources(theta, phi, nside, normalize=True)\n",
    "ax.imshow_hpx(counts, cmap=\"cylon\")\n",
    "ax.set_title(event)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648802e-8cce-435c-9594-4d42c3758e28",
   "metadata": {},
   "source": [
    "We estimate the sky localization uncertainty in two ways. First, we use the [`find_ellipse`](https://lscsoft.docs.ligo.org/ligo.skymap/postprocess/ellipse.html) method implemented in the `ligo.skymap` package. The method finds the ellipse containing a given probability, returning its area among other properties.\n",
    "\n",
    "We also compare this calculation with the (naive) greedy estimate of ranking HEALPix pixels by their counts and returning the smallest number that contains the desired fraction of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa6be10-e0b5-4758-a4d2-6a1669c9e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 90\n",
    "level_dec = level / 100\n",
    "projection = \"MOL\"\n",
    "ellipse = find_ellipse(counts, cl=level, projection=projection)\n",
    "n_pix = np.searchsorted(np.cumsum(np.sort(counts)[::-1]), level_dec)\n",
    "sky_localization_uncertainty = n_pix * area_per_pixel\n",
    "print(f\"Area at {level}% [Ellipse]: {ellipse[-1]:.2f} deg^2\")\n",
    "print(f\"Area at {level}% [HEALPix]: {sky_localization_uncertainty:.2f} deg^2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b96a4e-6d0b-4d98-8722-573e8e0205d4",
   "metadata": {},
   "source": [
    "## Distance distributions\n",
    "\n",
    "In the cell below, we compare the distance posteriors between analyses for each event. For the same outlier events that we presented above, the posteriors seem to be bounded by the prior. Therefore, we will remove them from the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b55e51-2a92-4929-99cf-ae9f4762242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(events), 1, figsize=(8, 1.5 * len(events)))\n",
    "dl_min = np.inf\n",
    "dl_max = -np.inf\n",
    "legend_set = False\n",
    "colors = [\"tab:orange\", \"tab:blue\", \"tab:green\"]\n",
    "for event, ax in zip(samples_file.keys(), axs):\n",
    "    event_group = samples_file[event]\n",
    "    handles = []\n",
    "    for approximant, color in zip(analyses, colors):\n",
    "        try:\n",
    "            label = approximant_to_label(approximant)\n",
    "            samples = event_group[label][\"luminosity_distance\"][()]\n",
    "            dl_min = min(dl_min, samples.min())\n",
    "            dl_max = max(dl_max, samples.max())\n",
    "            grid, pdf = az.kde(samples)\n",
    "            line, = ax.semilogx(grid, pdf, label=approximant, color=color)\n",
    "            handles.append(line)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if len(handles) == len(analyses) and not legend_set:\n",
    "        plt.figlegend(handles=handles, loc=\"upper right\")\n",
    "        legend_set = True\n",
    "    ax.grid()\n",
    "    ax.set(title=event, xlabel=r\"$D_L \\, [\\rm{Mpc}]$\", ylabel=\"PDF\")\n",
    "for ax in axs:\n",
    "    ax.set_xlim([dl_min, dl_max])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75217cff-d80f-4cbf-b659-efde54a668bd",
   "metadata": {},
   "source": [
    "Storing all localization samples for the corresponding `analysis` in a separate file, skipping outlier events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e013988-c6f2-4630-9a26-1d599c80d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_label = approximant_to_label(output_analysis)\n",
    "with h5py.File(path_to_output_file, mode=\"w\") as output_file:\n",
    "    for event, *confidence_areas in event_localization_table.iterrows(\"Event\", *confidence_area_labels):\n",
    "        if event in outlier_events:\n",
    "            continue\n",
    "        try:\n",
    "            group = samples_file[event]\n",
    "            group.copy(group[output_label], output_file, event, without_attrs=True)\n",
    "            confidence_areas = np.asarray([area.value for area in confidence_areas])\n",
    "            confidence_area_90 = confidence_areas[np.isfinite(confidence_areas)][-1]\n",
    "            output_file[event].attrs[\"confidence_area_90\"] = confidence_area_90\n",
    "        except KeyError:\n",
    "            print(f\"Analysis {output_analysis} not found for event {event}\") \n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
